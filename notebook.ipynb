{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Download the dataset from [here](https://www.kaggle.com/stackoverflow/stacksample)\n",
    "- For setting up the elastic instance I suggest to use a docker image.\n",
    "- Make sure you have docker installed.\n",
    "- Run the below command. This will launch an elastic instance\n",
    "- \n",
    "<code>docker run -p 8200:9200 -p 8600:9600 -e \"discovery.type=single-node\" amazon/opendistro-for-elasticsearch:1.8.0 \n",
    "</code>\n",
    "- Download the sentence encoder from [here](https://tfhub.dev/google/universal-sentence-encoder/4) \n",
    "- I have used the opendistro's elastic search as they provide inbuilt Approximate Nearest Neighbors implementation. It uses nmslib's implementation for HNSW\n",
    "- HNSW tops the [benchmarks](https://github.com/erikbern/ann-benchmarks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import time\n",
    "import sys\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import bulk\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import warnings\n",
    "from flask import jsonify\n",
    "from flask import Flask\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connecting to the elastic instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect2ES():\n",
    "    '''\n",
    "    This function creates a connection to the elastic search instance\n",
    "    we provide the appropriate host name, port number, and authentication\n",
    "    details.\n",
    "\n",
    "        '''\n",
    "    \n",
    "    es = Elasticsearch([{'host': 'localhost', 'port': '8200','use_ssl':True,'verify_certs':False}], http_auth=('admin', 'admin'))\n",
    "    if es.ping():\n",
    "            print('Connected to ES!')\n",
    "    else:\n",
    "            print('Could not connect!')\n",
    "    return es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the universal sentence encoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = hub.load(\"./use4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the index schema for elastic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def createSchema(es):\n",
    "        '''\n",
    "            Creates a new index in the elastic search. \n",
    "            Defines appropriate schema for the index.\n",
    "            We define \n",
    "                title         => text\n",
    "                title_vector  => knn_vector\n",
    "\n",
    "        '''\n",
    "\n",
    "        #Refer: https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping.html\n",
    "        # Mapping: Structure of the index\n",
    "        # Property/Field: name and type  \n",
    "\n",
    "        b = {\n",
    "          \"settings\": {\n",
    "            \"index\": {\n",
    "              \"knn\": True,\n",
    "              \"knn.space_type\": \"cosinesimil\"\n",
    "            }\n",
    "          },\n",
    "          \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"title\":{\n",
    "                    \"type\":\"text\"\n",
    "                },\n",
    "              \"title_vector\": {\n",
    "                \"type\": \"knn_vector\", # Helps to find approximate k nearest neighbours\n",
    "                \"dimension\": 512\n",
    "              }\n",
    "\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "\n",
    "        ret = es.indices.create(index='questions-index', ignore=400, body=b)\n",
    "        print(json.dumps(ret,indent=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data ingestion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataIngestion(es,NUM_QUESTIONS_INDEXED):\n",
    "    '''\n",
    "    This function helps us in ingesting the data into the\n",
    "    elastic search index\n",
    "    Here we index the title vector and the title.\n",
    "    '''\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Col-Names: Id,OwnerUserId,CreationDate,ClosedDate,Score,Title,Body\n",
    "    cnt=0\n",
    "\n",
    "    with open('Questions.csv', encoding=\"latin1\") as csvfile:\n",
    "        readCSV = csv.reader(csvfile, delimiter=',' )\n",
    "        next(readCSV, None)  # skip the headers \n",
    "        for row in (readCSV):\n",
    "            doc_id = row[0];\n",
    "            title = row[5];\n",
    "            body = row[6]\n",
    "            vec = tf.make_ndarray(tf.make_tensor_proto(embed([title]))).tolist()[0]\n",
    "\n",
    "            b = {\n",
    "                 \"title\":title,\n",
    "                 \"title_vector\":vec,\n",
    "                }\n",
    "\n",
    "            res = es.index(index=\"questions-index\", id=doc_id, body=b)\n",
    "\n",
    "            cnt += 1\n",
    "            if cnt%1000==0:\n",
    "                print(cnt)\n",
    "\n",
    "            if cnt == NUM_QUESTIONS_INDEXED:\n",
    "                break;\n",
    "\n",
    "        print(\"Completed indexing....\")\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    print('****************************************************************************')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search engine implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Search by Keywords\n",
    "def keywordSearch(es, q):\n",
    "    '''\n",
    "    Implements the traditional keyword search using inverted index.\n",
    "    Elastic search uses a TF-IDF based metric to rank the results.\n",
    "\n",
    "    '''\n",
    "\n",
    "    b={\n",
    "            'query':{\n",
    "                'match':{\n",
    "                    \"title\":q\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    res= es.search(index='questions-index',body=b)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search by Vec Similarity\n",
    "def sentenceSimilaritybyNN(es, sent):\n",
    "    '''\n",
    "    Implements an approximate nearest negihbours method \n",
    "    to find the k nearest vectors.\n",
    "    Elastic search relies nmslib for the implementation of HNSW\n",
    "    \n",
    "    https://github.com/nmslib/hnswlib\n",
    "    '''\n",
    "    \n",
    "    query_vector = tf.make_ndarray(tf.make_tensor_proto(embed([sent]))).tolist()[0]\n",
    "    kb={\n",
    "          \"size\": 10,\n",
    "          \"query\": {\n",
    "            \"knn\": {\n",
    "              \"title_vector\": {\n",
    "                \"vector\": query_vector,\n",
    "                \"k\": 10\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "\n",
    "    res= es.search(index='questions-index',body=kb,request_timeout=100)\n",
    "    \n",
    "    return res;\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchEngine():\n",
    "    '''\n",
    "        Search engine method which\n",
    "        shows results based on semantic similarity\n",
    "        as well as keyword based similarity\n",
    "        and shows the results in decreasing order of relevance\n",
    "    '''\n",
    "    es=connect2ES()\n",
    "    print(\"Please give the input query\")\n",
    "    query=input()\n",
    "    print('------------------------SEMANTIC RESULTS---------------------------')\n",
    "    res= sentenceSimilaritybyNN(es,query)\n",
    "    print(\"Semantic Similarity Search:\\n\")\n",
    "    for hit in res['hits']['hits']:\n",
    "        print(str(hit['_score']) + \"\\t\" + hit['_source']['title'] )\n",
    "    \n",
    "    print('')\n",
    "    \n",
    "    res= keywordSearch(es,query)\n",
    "    print(\"('------------------------KEYWORDS RESULTS---------------------------')\")\n",
    "    for hit in res['hits']['hits']:\n",
    "        print(str(hit['_score']) + \"\\t\" + hit['_source']['title'] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to ES!\n",
      "Please give the input query\n",
      "delete file in linux\n",
      "------------------------SEMANTIC RESULTS---------------------------\n",
      "Semantic Similarity Search:\n",
      "\n",
      "0.8078813\tRemoving a file in a Restricted Folder in Linux\n",
      "0.78672296\tremove certain tag in files under linux?\n",
      "0.780427\tMaillog file in linux\n",
      "0.7667524\tBash: Delete until a specific file\n",
      "0.75369155\tremove directory in c++\n",
      "0.7533658\tDelete a line from a file in java\n",
      "0.7423169\tDelete unused files\n",
      "0.7415796\tDeleting files in higher directory\n",
      "0.74069196\tRemove a symlink to a directory\n",
      "0.7325041\tHow to make files in Linux folder with default group write permission\n",
      "\n",
      "('------------------------KEYWORDS RESULTS---------------------------')\n",
      "12.563389\tMaillog file in linux\n",
      "11.829328\tFile paths in Java (Linux)\n",
      "11.598585\tLinux File Logs\n",
      "11.566038\tDelete an sdf file in use?\n",
      "11.176312\tmd5sum of file in Linux C\n",
      "10.960959\tDelete file without playing sound in Applescript?\n",
      "10.746357\tMy delete function does not delete the targeted file\n",
      "10.6557045\tTortoise Delete File System Repository\n",
      "10.416043\tDelete a line from a file in java\n",
      "10.416043\tDelete a character from a file in C\n"
     ]
    }
   ],
   "source": [
    "searchEngine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying using flask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_server():\n",
    "    '''\n",
    "        This function exposes our search engine using flask.\n",
    "        The server is hosted on locahost:5005\n",
    "        We perform GET request \n",
    "        http://localhost:5005/search/recursion+vs+iteration\n",
    "        use '+' as a seperator\n",
    "    '''\n",
    "    app = Flask(__name__)\n",
    "    es = connect2ES();\n",
    "    # embed = hub.load(\"./data/USE4/\")\n",
    "\n",
    "    @app.route('/search/<query>')\n",
    "    def search(query):\n",
    "        q = query.replace(\"+\", \" \")\n",
    "        res_kw = keywordSearch(es, q)\n",
    "        res_semantic = sentenceSimilaritybyNN( es, q)\n",
    "\n",
    "        result=[]\n",
    "        result.append('------------------------SEMANTIC RESULTS---------------------------')\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for i in res_semantic['hits']['hits']:\n",
    "            result.append(i['_source']['title'])\n",
    "        \n",
    "        result.append('------------------------KEYWORDS RESULTS---------------------------')\n",
    "        \n",
    "        for i in res_kw['hits']['hits']:\n",
    "            result.append(i['_source']['title'])\n",
    "\n",
    "        timeTaken = time.time() - start_time\n",
    "        return {\"result\":result,\"time_taken\":timeTaken}\n",
    "        return {\"message\": \"Hello World\"}\n",
    "    \n",
    "    app.run(host='0.0.0.0',port=5005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final():\n",
    "    '''\n",
    "        This runs the entire pipeline for the project.\n",
    "        Right from data ingestion to exposing the API\n",
    "    \n",
    "    '''\n",
    "    es=connect2ES()\n",
    "    createSchema(es)\n",
    "    dataIngestion(es,100) # number of documents to be ingested\n",
    "    print(\"Status 400 denotes index already exists\")\n",
    "    print(\"Enter a search query\")\n",
    "    start_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to ES!\n",
      "{\n",
      "    \"error\": {\n",
      "        \"root_cause\": [\n",
      "            {\n",
      "                \"type\": \"resource_already_exists_exception\",\n",
      "                \"reason\": \"index [questions-index/AZJN_HWBRZ2ZjS69ALoUoQ] already exists\",\n",
      "                \"index_uuid\": \"AZJN_HWBRZ2ZjS69ALoUoQ\",\n",
      "                \"index\": \"questions-index\"\n",
      "            }\n",
      "        ],\n",
      "        \"type\": \"resource_already_exists_exception\",\n",
      "        \"reason\": \"index [questions-index/AZJN_HWBRZ2ZjS69ALoUoQ] already exists\",\n",
      "        \"index_uuid\": \"AZJN_HWBRZ2ZjS69ALoUoQ\",\n",
      "        \"index\": \"questions-index\"\n",
      "    },\n",
      "    \"status\": 400\n",
      "}\n",
      "Completed indexing....\n",
      "--- 2.7990684509277344 seconds ---\n",
      "****************************************************************************\n",
      "Status 400 denotes index already exists\n",
      "Enter a search query\n",
      "Connected to ES!\n",
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://0.0.0.0:5005/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [07/Jul/2020 22:33:16] \"\u001b[37mGET /search/delete+file+linux HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [07/Jul/2020 22:33:28] \"\u001b[37mGET /search/aws+ec2 HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    }
   ],
   "source": [
    "final()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
